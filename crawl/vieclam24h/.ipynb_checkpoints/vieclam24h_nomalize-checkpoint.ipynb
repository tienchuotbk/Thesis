{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370fa7cc-6e16-4abe-978f-9472fe05db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 22:53:51 WARN Utils: Your hostname, chuot-HP-Pavilion-Laptop-14-ce3xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.5 instead (on interface wlo1)\n",
      "24/05/29 22:53:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/29 22:53:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/29 22:53:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')  # Adjust the path if Spark is installed elsewhere\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6d42a9-d8fc-4066-a8a6-a6f15ab149ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the path to your JSON file\n",
    "from pyspark.sql.functions import col, udf, to_date, when, regexp_extract, explode, col, regexp_replace, to_timestamp, concat, lit, trim\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, ArrayType\n",
    "import json\n",
    "import re\n",
    "import pymongo\n",
    "json_file_path = 'vieclam24h.json'\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "\n",
    "# df_exploded = df.select(explode(col(\"location\")).alias(\"unique_category\"))\n",
    "\n",
    "# unique_categories = df_exploded.select(\"unique_category\").distinct().collect()\n",
    "# print(len(unique_categories))\n",
    "# for row in unique_categories:\n",
    "#     pass\n",
    "#     print(row.unique_category)\n",
    "# unique_experience_df = df.select(\"location\").distinct()\n",
    "\n",
    "# unique_experience_df.show()\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "# unique_values = df.select(col(\"experience\")).distinct().collect()\n",
    "\n",
    "# # In ra các giá trị duy nhất\n",
    "# for row in unique_values:\n",
    "#     if row.experience is None:\n",
    "#         print(\"None ne\")\n",
    "#     print(row.experience)\n",
    "\n",
    "\n",
    "# print(df.size)\n",
    "\n",
    "\n",
    "#rdd.toDF().select(\"salary\").show(1)\n",
    "\n",
    "# Salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a971ac-af9d-454f-94ef-472ee6a852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary\n",
    "def parse_salary(salary):\n",
    "    if salary is None:\n",
    "        return {\"type\": 0 }\n",
    "    elif (salary.strip().lower() == \"thoả thuận\" or salary.strip().lower() == \"thỏa thuận\"):\n",
    "        return {\"type\": 3 }\n",
    "    elif \"triệu\" in salary:\n",
    "        salary = salary.replace(\" triệu\", \"\").replace(\",\", \".\").strip()\n",
    "        if \"-\" in salary:\n",
    "            min_salary, max_salary = salary.split(\"-\")\n",
    "            return {\"type\": 1, \"min\": float(min_salary), \"max\": float(max_salary) }\n",
    "        else:\n",
    "            return {\"type\": 2, \"fixed_value\": float(salary)}\n",
    "# Age\n",
    "def parse_age(age):\n",
    "    if age is None:\n",
    "        return { \"type\": 0 }\n",
    "    age = age.strip().lower()\n",
    "    if \"tuổi\" in age:\n",
    "        age = age.replace(\" tuổi\", \"\").strip()\n",
    "        if \"-\" in age:\n",
    "            min_age, max_age = age.split(\"-\")\n",
    "            return { \"type\": 1, \"min\": int(min_age), \"max\": int(max_age) }\n",
    "        else:\n",
    "            return { \"type\": 2, \"value\": int(age) }\n",
    "    return { \"type\": 0 }\n",
    "\n",
    "# Experience\n",
    "def transform_experience(df):\n",
    "    df = df.withColumn(\"experience\", when(col(\"experience\") == \"Chưa có kinh nghiệm\", 0).otherwise(col(\"experience\")))\n",
    "    df = df.withColumn(\"experience\", regexp_replace(col(\"experience\"), \"(Dưới |Hơn |năm)\", \"\"))\n",
    "    df = df.withColumn(\"experience\", trim(col(\"experience\")))\n",
    "    df = df.withColumn(\"experience\", when(col(\"experience\") != \"0\", col(\"experience\").cast(\"int\") * 12).otherwise(0))\n",
    "    return df\n",
    "\n",
    "df = transform_experience(df)\n",
    "\n",
    "# Type\n",
    "df = df.withColumn(\n",
    "    \"type\",\n",
    "     when(col(\"type\") == \"Toàn thời gian cố định\", 0)\n",
    "    .when(col(\"type\") == \"Bán thời gian cố định\", 1)\n",
    "    .when(col(\"type\") == \"Toàn thời gian tạm thời\", 2)\n",
    "    .when(col(\"type\") == \"Khác\", 3)\n",
    "    .otherwise(3)\n",
    ")\n",
    "\n",
    "# Expiration\n",
    "df = df.withColumn(\"expiration\", to_timestamp(concat(col(\"expiration\"), lit(\" 00:00:00\")), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "# Update at\n",
    "df = df.withColumn(\"update_time\", to_timestamp(concat(col(\"update_time\"), lit(\" 00:00:00\")), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "# Description\n",
    "def clean_description(description):\n",
    "    return [re.sub(r'^[\\-\\•\\d\\.\\s]+', '', desc).strip() for desc in description]\n",
    "clean_description_udf = udf(clean_description, ArrayType(StringType()))\n",
    "df = df.withColumn(\"description\", clean_description_udf(col(\"description\")))\n",
    "\n",
    "# Role\n",
    "def classify_role(role):\n",
    "    if role == \"Chuyên viên- nhân viên\":\n",
    "        return 1\n",
    "    elif role == \"Quản lý cấp cao\":\n",
    "        return 2\n",
    "    elif role == \"Cộng tác viên\":\n",
    "        return 3\n",
    "    elif role == \"Chuyên gia\":\n",
    "        return 4\n",
    "    elif role == \"Quản lý nhóm- giám sát\":\n",
    "        return 5\n",
    "    elif role == \"Quản lý cấp trung\":\n",
    "        return 6\n",
    "    else:\n",
    "        return 0\n",
    "classify_role_udf = udf(classify_role, IntegerType())\n",
    "df = df.withColumn(\"role\", classify_role_udf(col(\"role\")))\n",
    "\n",
    "def normalize_benefit(benefits):\n",
    "    return list(map(lambda benefit: re.sub(r'^[*\\-+]+', '', benefit).strip(), benefits))\n",
    "\n",
    "benefit_udf = udf(normalize_benefit, ArrayType(StringType()))\n",
    "df = df.withColumn(\"benefit\", benefit_udf(col(\"benefit\")))\n",
    "df = df.withColumn(\"requirement\", benefit_udf(col(\"requirement\")))\n",
    "\n",
    "# location\n",
    "def normalize_location(locations):\n",
    "    arr = []\n",
    "    for location in locations:\n",
    "        parts = [part.strip() for part in location.split(',') if part.strip()]\n",
    "        if len(parts) >= 2:\n",
    "            province = parts[-1]\n",
    "            district = parts[-2]\n",
    "            address = ', '.join(parts[:-2])\n",
    "            arr.append({\"province\": province, \"district\": district, \"address\": address})\n",
    "        elif len(parts) == 1:\n",
    "            arr.append ({\"province\": parts[-1], \"district\": None, \"address\": None })\n",
    "        else: \n",
    "            arr.append({\"address\": None, \"district\": None, \"province\": None})\n",
    "    return arr\n",
    "    \n",
    "rdd = df.rdd.map(lambda row: {\n",
    "    **row.asDict(),\n",
    "    \"salary\": parse_salary(row[\"salary\"]),\n",
    "    \"age\": parse_age(row[\"age\"]),\n",
    "    \"location\": normalize_location(row[\"location\"])\n",
    "})\n",
    "\n",
    "\n",
    "def insert_into_mongodb(partition):\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\", username='admin', password='20194856')\n",
    "    db = client[\"thesis\"]\n",
    "    collection = db[\"jobs\"]\n",
    "\n",
    "    for record in partition:\n",
    "        collection.insert_one(record)\n",
    "\n",
    "# Insert dữ liệu vào collection \"jobs\"\n",
    "rdd.foreachPartition(insert_into_mongodb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c7a90-9d72-4f05-8a50-256723c19fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
