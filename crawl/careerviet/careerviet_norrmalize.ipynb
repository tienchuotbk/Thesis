{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370fa7cc-6e16-4abe-978f-9472fe05db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 22:13:52 WARN Utils: Your hostname, chuot-HP-Pavilion-Laptop-14-ce3xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.14 instead (on interface wlo1)\n",
      "24/07/05 22:13:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/05 22:13:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/05 22:13:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')  # Adjust the path if Spark is installed elsewhere\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6d42a9-d8fc-4066-a8a6-a6f15ab149ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nữ\n",
      "Nam\n",
      "None ne\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the path to your JSON file\n",
    "from pyspark.sql.functions import col, udf, to_date, when, regexp_extract, explode, col, regexp_replace, to_timestamp, concat, lit, trim, date_format\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, ArrayType\n",
    "import json\n",
    "import re\n",
    "import pymongo\n",
    "json_file_path = 'careerviet.json'\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "\n",
    "\n",
    "# df_exploded = df.select(explode(col(\"category\")).alias(\"unique_category\"))\n",
    "\n",
    "# unique_categories = df_exploded.select(\"unique_category\").distinct().collect()\n",
    "# print(len(unique_categories))\n",
    "# for row in unique_categories:\n",
    "#     pass\n",
    "#     print(row.unique_category)\n",
    "# unique_experience_df = df.select(\"location\").distinct()\n",
    "\n",
    "# unique_experience_df.show()\n",
    "# from pyspark.sql.functions import col\n",
    "unique_values = df.select(col(\"sex\")).distinct().collect()\n",
    "# unique_values.show()\n",
    "# In ra các giá trị duy nhất\n",
    "for row in unique_values:\n",
    "    if row.sex is None:\n",
    "        print(\"None ne\")\n",
    "    print(row.sex)\n",
    "\n",
    "\n",
    "# print(df.size)\n",
    "\n",
    "\n",
    "#rdd.toDF().select(\"salary\").show(1)\n",
    "\n",
    "# Salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "666e7672-3809-48db-90e8-2ec08aaf475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11521\n",
      "8531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of the first half: 3457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of the second half: 2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 418:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of the third half: 2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "print(df.count())\n",
    "df = df.dropDuplicates([\"title\"])\n",
    "print(df.count())\n",
    "# df.write.json(\"careervietclean.json\", mode='overwrite')\n",
    "df1, df2, df3 = df.randomSplit([0.4, 0.3, 0.3], seed=42)\n",
    "print(f\"Count of the first half: {df1.count()}\")\n",
    "print(f\"Count of the second half: {df2.count()}\")\n",
    "print(f\"Count of the third half: {df3.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6a971ac-af9d-454f-94ef-472ee6a852b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No match:\"Quốc tế\"\n",
      "No match:\"Đồng Bằng Sông Cửu Long\"\n",
      "No match:\"KV Tây Nguyên\"\n",
      "No match:\"KV Đông Nam Bộ\"\n",
      "No match:\"Quốc tế\"\n",
      "No match:\"Khác\"\n",
      "No match:\"KV Đông Nam Bộ\"\n",
      "No match:\"Quốc tế\"\n",
      "No match:\"Malaysia\"                                                 (0 + 1) / 1]\n",
      "No match:\"Khác\"\n",
      "No match:\"Đồng Bằng Sông Cửu Long\"\n",
      "24/07/05 23:59:39 ERROR Executor: Exception in task 0.0 in stage 452.0 (TID 261)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/tmp/ipykernel_6883/462449540.py\", line 491, in insert_into_mongodb\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6883/462449540.py\", line 473, in <lambda>\n",
      "  File \"/tmp/ipykernel_6883/462449540.py\", line 82, in parse_experience\n",
      "ValueError: too many values to unpack (expected 2)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/07/05 23:59:39 WARN TaskSetManager: Lost task 0.0 in stage 452.0 (TID 261) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/tmp/ipykernel_6883/462449540.py\", line 491, in insert_into_mongodb\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6883/462449540.py\", line 473, in <lambda>\n",
      "  File \"/tmp/ipykernel_6883/462449540.py\", line 82, in parse_experience\n",
      "ValueError: too many values to unpack (expected 2)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/07/05 23:59:39 ERROR TaskSetManager: Task 0 in stage 452.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 452.0 failed 1 times, most recent failure: Lost task 0.0 in stage 452.0 (TID 261) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 491, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 473, in <lambda>\n  File \"/tmp/ipykernel_6883/462449540.py\", line 82, in parse_experience\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 491, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 473, in <lambda>\n  File \"/tmp/ipykernel_6883/462449540.py\", line 82, in parse_experience\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6883/462449540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;31m# Insert dữ liệu vào collection \"jobs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsert_into_mongodb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Insert sucess to mongodb: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" jobs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mforeachPartition\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[T]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \"\"\"\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \"\"\"\n\u001b[0;32m-> 2291\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2292\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         )\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 452.0 failed 1 times, most recent failure: Lost task 0.0 in stage 452.0 (TID 261) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 491, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 473, in <lambda>\n  File \"/tmp/ipykernel_6883/462449540.py\", line 82, in parse_experience\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 491, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6883/462449540.py\", line 473, in <lambda>\n  File \"/tmp/ipykernel_6883/462449540.py\", line 82, in parse_experience\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df = df3\n",
    "\n",
    "df = df.filter(col(\"company\").isNotNull())\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"certificate\",\n",
    "     when(col(\"certificate\") == None, 0)\n",
    "    .when(col(\"certificate\") == \"Chứng chỉ\", 1)\n",
    "    .when(col(\"certificate\") == \"Trung học\", 2)\n",
    "    .when(col(\"certificate\") == \"Trung cấp\", 3)\n",
    "    .when(col(\"certificate\") == \"Cao đẳng\", 4)\n",
    "    .when(col(\"certificate\") == \"Đại học\", 5)\n",
    "    .when(col(\"certificate\") == \"Sau đại học\", 6)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"sex\",\n",
    "     when(col(\"sex\") == \"Nam\", 1)\n",
    "    .when(col(\"sex\") == \"Nữ\", 2)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Salary\n",
    "def parse_salary(salary):\n",
    "    vnd_per_usd = 25\n",
    "    if salary is None:\n",
    "        return {\"type\": 0 }\n",
    "    elif (salary.strip().lower() == \"cạnh tranh\"):\n",
    "        return {\"type\": 3 }\n",
    "    elif \"VND\" in salary:\n",
    "            salary = salary.replace(\" VND\", \"\").replace(\" Tr\", \"\").replace(\",\", \".\").strip()\n",
    "            if \"-\" in salary:\n",
    "                min_salary, max_salary = salary.split(\"-\")\n",
    "                return {\"type\": 1, \"min\": float(min_salary), \"max\": float(max_salary) }\n",
    "            elif \"Trên\" in salary:\n",
    "                min_salary = salary.replace(\"Trên\", \"\").strip()\n",
    "                return {\"type\": 5, \"min\": float(min_salary)}\n",
    "            elif \"Lên đến\" in salary:\n",
    "                max_salary = salary.replace(\"Lên đến\", \"\").strip()\n",
    "                return {\"type\": 4, \"max\": float(max_salary)}\n",
    "    elif \"USD\" in salary:\n",
    "        salary = salary.replace(\" USD\", \"\").replace(\" Tr\", \"\").replace(\",\", \"\").strip()\n",
    "        if \"-\" in salary:\n",
    "            min_salary, max_salary = salary.split(\"-\")\n",
    "            return {\"type\": 1, \"min\": float(min_salary)* vnd_per_usd / 1000, \"max\": float(max_salary)* vnd_per_usd /1000}\n",
    "        elif \"Trên\" in salary:\n",
    "            min_salary = salary.replace(\"Trên\", \"\").strip()\n",
    "            return {\"type\": 5, \"min\": float(min_salary)* vnd_per_usd / 1000}\n",
    "        elif \"Lên đến\" in salary:\n",
    "            max_salary = salary.replace(\"Lên đến\", \"\").strip()\n",
    "            return {\"type\": 4, \"min\": float(max_salary)* vnd_per_usd / 1000}\n",
    "    else:\n",
    "        return {\"type\": 0 }\n",
    "                \n",
    "# Age\n",
    "def parse_age(age):\n",
    "    if age is None or age == \"Không giới hạn tuổi\":\n",
    "        return { \"type\": 0 }\n",
    "    # age = age.strip().lower()\n",
    "    if \"-\" in age:\n",
    "        strip_age = age.strip()\n",
    "        min_age, max_age = age.strip().split(\"-\")\n",
    "        return { \"type\": 1, \"min\": int(min_age), \"max\": int(max_age) }\n",
    "    elif \"Trên\" in age:\n",
    "        min_age = age.replace(\"Trên \", \"\")\n",
    "        return { \"type\": 4, \"min\": int(min_age)}\n",
    "    elif \"Dưới\" in age:\n",
    "        max_age = age.replace(\"Dưới \", \"\")\n",
    "        return { \"type\": 3, \"min\": int(max_age)}  \n",
    "    else:\n",
    "        fixed = age.strip()\n",
    "        return { \"type\": 2, \"fixed\": int(fixed)}  \n",
    "    return { \"type\": 0 }\n",
    "\n",
    "# Experience\n",
    "def parse_experience(experience):\n",
    "    if experience is None or experience == \"Chưa có kinh nghiệm\":\n",
    "        return { \"type\": 0 }\n",
    "    if \"-\" in experience:\n",
    "        strip_experience = experience.replace(\"Năm\", \"\").strip()\n",
    "        min_exp, max_exp = strip_experience.split(\"-\")\n",
    "        return {\"type\": 1, \"min\": int(min_exp), \"max\": int(max_exp) }\n",
    "    elif \"Lên đến\" in experience:\n",
    "        strip_experience = experience.replace(\"Lên đến\", \"\").replace(\"Năm\", \"\").strip()\n",
    "        max = strip_experience\n",
    "        return { \"type\": 3, \"max\": int(max) }\n",
    "    elif \"Trên\" in experience:\n",
    "        strip_experience = experience.replace(\"Trên\", \"\").replace(\"Năm\", \"\").strip()\n",
    "        min = strip_experience\n",
    "        return { \"type\": 4, \"min\": int(min) }\n",
    "    else:\n",
    "        fixed = experience.replace(\"Năm\", \"\").strip()\n",
    "        return { \"type\": 2, \"fixed\": int(fixed)}\n",
    "    return { \"type\": 0 }\n",
    "\n",
    "# Expiration\n",
    "df = df.withColumn(\"expiration\", date_format(to_timestamp(concat(col(\"expiration\"), lit(\" 00:00:00\")), \"dd/MM/yyyy HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Update at\n",
    "df = df.withColumn(\"update_time\", date_format(to_timestamp(concat(col(\"update_time\"), lit(\" 00:00:00\")), \"dd/MM/yyyy HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Type\n",
    "def get_type_job(type):\n",
    "    if type == \"Nhân viên chính thức\":\n",
    "        return 0\n",
    "    elif type == \"Bán thời gian\":\n",
    "        return 2\n",
    "    elif type == \"Thời vụ/ Nghề tự do\":\n",
    "        return 1\n",
    "    elif type == \"Thực tập\":\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "def get_list_type_job(types):\n",
    "    # Thời vụ/ Nghề tự do, Thực tập\n",
    "    # type = types.split(\",\")\n",
    "    if types is None:\n",
    "        return []\n",
    "    return [get_type_job(word.strip()) for word in types.split(\",\")]\n",
    "\n",
    "\n",
    "classify_type_udf = udf(get_list_type_job, ArrayType(IntegerType()))\n",
    "df = df.withColumn(\"type\", classify_type_udf(col(\"type\")))\n",
    "\n",
    "# Role\n",
    "def classify_role(role):\n",
    "    if role is None:\n",
    "        return 0\n",
    "    role = role.strip()\n",
    "    if role == \"Nhân viên\":\n",
    "        return 0\n",
    "    elif role == \"Quản lý\" or role == \"Quản lý cấp trung\":\n",
    "        return 1\n",
    "    elif role == \"Cộng tác viên\":\n",
    "        return 6\n",
    "    elif role == \"Trưởng nhóm / Giám sát\":\n",
    "        return 5\n",
    "    elif role == \"Sinh viên/ Thực tập sinh\" or role == \"Mới tốt nghiệp\":\n",
    "        return 4\n",
    "    elif role == \"Giám đốc\" or role == \"Tổng giám đốc\":\n",
    "        return 2\n",
    "    elif role == \"Phó Giám đốc\":\n",
    "        return 3\n",
    "    elif role == \"Chuyên gia\":\n",
    "        return 7\n",
    "    else:\n",
    "        return 0\n",
    "classify_role_udf = udf(classify_role, IntegerType())\n",
    "df = df.withColumn(\"role\", classify_role_udf(col(\"role\")))\n",
    "\n",
    "def normalize_benefit(benefits):\n",
    "    if benefits is None:\n",
    "        return []\n",
    "    return list(map(lambda benefit: re.sub(r'^[*\\-+]+', '', benefit).strip(), benefits))\n",
    "benefit_udf = udf(normalize_benefit, ArrayType(StringType()))\n",
    "df = df.withColumn(\"benefit\", benefit_udf(col(\"benefit\")))\n",
    "df = df.withColumn(\"requirement\", benefit_udf(col(\"requirement\")))\n",
    "df = df.withColumn(\"description\", benefit_udf(col(\"requirement\")))\n",
    "\n",
    "def getProvince(province):\n",
    "    if province == \"An Giang\": \n",
    "      location = \"AG\"\n",
    "      \n",
    "    elif province == \"Bà Rịa - Vũng Tàu\": \n",
    "      location = \"BV\"\n",
    "      \n",
    "    elif province == \"Bạc Liêu\": \n",
    "      location = \"BL\"\n",
    "      \n",
    "    elif province == \"Bắc Kạn\" or province == 'Bắc Cạn': \n",
    "      location = \"BK\"\n",
    "      \n",
    "    elif province == \"Bắc Giang\": \n",
    "      location = \"BG\"\n",
    "      \n",
    "    elif province == \"Bắc Ninh\": \n",
    "      location = \"BN\"\n",
    "      \n",
    "    elif province == \"Bến Tre\": \n",
    "      location = \"BT\"\n",
    "      \n",
    "    elif province == \"Bình Dương\": \n",
    "      location = \"BD\"\n",
    "      \n",
    "    elif province == \"Bình Định\": \n",
    "      location = \"BDI\"\n",
    "      \n",
    "    elif province == \"Bình Phước\": \n",
    "      location = \"BP\"\n",
    "      \n",
    "    elif province == \"Bình Thuận\": \n",
    "      location = \"BT\"\n",
    "      \n",
    "    elif province == \"Cà Mau\": \n",
    "      location = \"CM\"\n",
    "      \n",
    "    elif province == \"Cao Bằng\": \n",
    "      location = \"CB\"\n",
    "      \n",
    "    elif province == \"Cần Thơ\": \n",
    "      location = \"CT\"\n",
    "      \n",
    "    elif province == \"Đà Nẵng\": \n",
    "      location = \"DN\"\n",
    "      \n",
    "    elif province == \"Đắk Lắk\" or province == 'Dak Lak': \n",
    "      location = \"DL\"\n",
    "      \n",
    "    elif province == \"Đắk Nông\" or province == 'Dak Nông': \n",
    "      location = \"DNO\"\n",
    "      \n",
    "    elif province == \"Điện Biên\": \n",
    "      location = \"DB\"\n",
    "      \n",
    "    elif province == \"Đồng Nai\": \n",
    "      location = \"DNA\"\n",
    "      \n",
    "    elif province == \"Đồng Tháp\": \n",
    "      location = \"DT\"\n",
    "      \n",
    "    elif province == \"Gia Lai\": \n",
    "      location = \"GL\"\n",
    "      \n",
    "    elif province == \"Hà Giang\": \n",
    "      location = \"HG\"\n",
    "      \n",
    "    elif province == \"Hà Nam\": \n",
    "      location = \"HNA\"\n",
    "      \n",
    "    elif province == \"Hà Nội\": \n",
    "      location = \"HN\"\n",
    "      \n",
    "    elif province == \"Hà Tĩnh\": \n",
    "      location = \"HT\"\n",
    "      \n",
    "    elif province == \"Hải Dương\": \n",
    "      location = \"HD\"\n",
    "      \n",
    "    elif province == \"Hải Phòng\": \n",
    "      location = \"HP\"\n",
    "      \n",
    "    elif province == \"Hậu Giang\": \n",
    "      location = \"HGI\"\n",
    "      \n",
    "    elif province == \"Hòa Bình\": \n",
    "      location = \"HB\"\n",
    "\n",
    "    elif province == 'Hồ Chí Minh' or province == \"TP.HCM\" : \n",
    "      location = \"HCM\"\n",
    "      \n",
    "    elif province == \"Hưng Yên\": \n",
    "      location = \"HY\"\n",
    "      \n",
    "    elif province == \"Khánh Hòa\": \n",
    "      location = \"KH\"\n",
    "      \n",
    "    elif province == \"Kiên Giang\": \n",
    "      location = \"KG\"\n",
    "      \n",
    "    elif province == \"Kon Tum\": \n",
    "      location = \"KT\"\n",
    "      \n",
    "    elif province == \"Lai Châu\": \n",
    "      location = \"LC\"\n",
    "      \n",
    "    elif province == \"Lạng Sơn\": \n",
    "      location = \"LS\"\n",
    "      \n",
    "    elif province == \"Lào Cai\": \n",
    "      location = \"LCA\"\n",
    "      \n",
    "    elif province == \"Lâm Đồng\": \n",
    "      location = \"LD\"\n",
    "      \n",
    "    elif province == \"Long An\": \n",
    "      location = \"LA\"\n",
    "      \n",
    "    elif province == \"Nam Định\": \n",
    "      location = \"ND\"\n",
    "      \n",
    "    elif province == \"Nghệ An\": \n",
    "      location = \"NA\"\n",
    "      \n",
    "    elif province == \"Ninh Bình\": \n",
    "      location = \"NB\"\n",
    "      \n",
    "    elif province == \"Ninh Thuận\": \n",
    "      location = \"NT\"\n",
    "      \n",
    "    elif province == \"Phú Thọ\": \n",
    "      location = \"PT\"\n",
    "      \n",
    "    elif province == \"Phú Yên\": \n",
    "      location = \"PY\"\n",
    "      \n",
    "    elif province == \"Quảng Bình\": \n",
    "      location = \"QB\"\n",
    "      \n",
    "    elif province == \"Quảng Nam\": \n",
    "      location = \"QNA\"\n",
    "      \n",
    "    elif province == \"Quảng Ngãi\": \n",
    "      location = \"QNG\"\n",
    "      \n",
    "    elif province == \"Quảng Ninh\": \n",
    "      location = \"QN\"\n",
    "      \n",
    "    elif province == \"Quảng Trị\": \n",
    "      location = \"QT\"\n",
    "      \n",
    "    elif province == \"Sóc Trăng\": \n",
    "      location = \"ST\"\n",
    "      \n",
    "    elif province == \"Sơn La\": \n",
    "      location = \"SL\"\n",
    "      \n",
    "    elif province == \"Tây Ninh\": \n",
    "      location = \"TN\"\n",
    "      \n",
    "    elif province == \"Thái Bình\": \n",
    "      location = \"TB\"\n",
    "      \n",
    "    elif province == \"Thái Nguyên\": \n",
    "      location = \"TNG\"\n",
    "      \n",
    "    elif province == \"Thanh Hóa\": \n",
    "      location = \"TH\"\n",
    "      \n",
    "    elif province == \"Thừa Thiên- Huế\" or province == \"Thừa Thiên Huế\": \n",
    "      location = \"TTH\"\n",
    "      \n",
    "    elif province == \"Tiền Giang\": \n",
    "      location = \"TG\"\n",
    "      \n",
    "    elif province == \"Trà Vinh\": \n",
    "      location = \"TV\"\n",
    "      \n",
    "    elif province == \"Tuyên Quang\": \n",
    "      location = \"TQ\"\n",
    "      \n",
    "    elif province == \"Vĩnh Long\": \n",
    "      location = \"VL\"\n",
    "      \n",
    "    elif province == \"Vĩnh Phúc\": \n",
    "      location = \"VP\"\n",
    "      \n",
    "    elif province == \"Yên Bái\": \n",
    "      location = \"YB\"\n",
    "    else:\n",
    "        print(\"No match:\\\"\"+ province+ \"\\\"\")\n",
    "        location = 'other'\n",
    "    return location\n",
    "\n",
    "# location\n",
    "def normalize_location(provinces, locations):\n",
    "    arr = []\n",
    "    if provinces is None:\n",
    "        provinces = []\n",
    "    if locations is None:\n",
    "        locations = []\n",
    "    if len(locations) == 0 and len(provinces) == 0:\n",
    "        return []\n",
    "    if len(locations) == 0 and len(provinces) > 0:\n",
    "        for province in provinces:\n",
    "            arr.append({\"province\": getProvince(province), \"district\": None, \"address\": province})\n",
    "    else:   \n",
    "        for province, location in zip(provinces, locations):\n",
    "            parts = [part.strip() for part in location.split(',') if part.strip()]\n",
    "            if len(parts) >= 2:\n",
    "                job_province = parts[-1]\n",
    "                district = None\n",
    "                if job_province == province:\n",
    "                    district = parts[-2]\n",
    "                else:\n",
    "                    district = [address for address in parts if 'huyện' in address.lower() or 'Q.' in address or 'Quận' in address.lower()]\n",
    "                arr.append({\"province\": getProvince(province), \"district\": district, \"address\": location})\n",
    "            else: \n",
    "                arr.append({\"address\": location, \"district\": None, \"province\": getProvince(province)})\n",
    "    return arr\n",
    "\n",
    "category_mapping = {\n",
    "    \"Du lịch\": \"dv\",\n",
    "    \"Thủy lợi\": \"nn_ln_ts\",\n",
    "    \"Công nghệ thực phẩm / Dinh dưỡng\": \"tp\",\n",
    "    \"Lâm Nghiệp\": \"nn_ln_ts\",\n",
    "    \"In ấn / Xuất bản\": \"xb\",\n",
    "    \"An toàn lao động\": \"at_an\",\n",
    "    \"Hành chính / Thư ký\": \"hc_ql\",\n",
    "    \"CNTT - Phần mềm\": \"it\",\n",
    "    \"Quảng cáo / Đối ngoại / Truyền Thông\": \"tt_mkt\",\n",
    "    \"Nông nghiệp\": \"nn_ln_ts\",\n",
    "    \"Ngân hàng\": \"tc_kt\",\n",
    "    \"Quản lý điều hành\": \"hc_ql\",\n",
    "    \"Kiến trúc\": \"tk_kt_nt\",\n",
    "    \"Mỹ thuật / Nghệ thuật / Thiết kế\": \"tk_kt_nt\",\n",
    "    \"Trắc địa / Địa Chất\": \"kh_kt\",\n",
    "    \"Điện / Điện tử / Điện lạnh\": \"kh_kt\",\n",
    "    \"Hàng gia dụng / Chăm sóc cá nhân\": \"kd\",\n",
    "    \"Chứng khoán\": \"tc_kt\",\n",
    "    \"Bảo trì / Sửa chữa\": \"cn_sx\",\n",
    "    \"Tổ chức sự kiện\": \"dv\",\n",
    "    \"Sản xuất / Vận hành sản xuất\": \"cn_sx\",\n",
    "    \"Nhà hàng / Khách sạn\": \"dv\",\n",
    "    \"Hàng không\": \"vt\",\n",
    "    \"Luật / Pháp lý\": \"law\",\n",
    "    \"Bất động sản\": \"bds\",\n",
    "    \"Tư vấn\": \"dv\",\n",
    "    \"Dịch vụ khách hàng\": \"dv\",\n",
    "    \"Y tế / Chăm sóc sức khỏe\": \"yt_sk\",\n",
    "    \"Công nghệ sinh học\": \"kh_kt\",\n",
    "    \"Giáo dục / Đào tạo\": \"gd_dt\",\n",
    "    \"Chăn nuôi / Thú y\": \"nn_ln_ts\",\n",
    "    \"An Ninh / Bảo Vệ\": \"at_an\",\n",
    "    \"CNTT - Phần cứng / Mạng\": \"it\",\n",
    "    \"Dầu khí\": \"cn_sx\",\n",
    "    \"Môi trường\": \"cn_sx\",\n",
    "    \"Bán lẻ / Bán sỉ\": \"kd\",\n",
    "    \"Tài chính / Đầu tư\": \"tc_kt\",\n",
    "    \"Thủy sản / Hải sản\": \"nn_ln_ts\",\n",
    "    \"Phi chính phủ / Phi lợi nhuận\": \"dv\",\n",
    "    \"Kế toán / Kiểm toán\": \"tc_kt\",\n",
    "    \"Bán hàng / Kinh doanh\": \"kd\",\n",
    "    \"Xuất nhập khẩu\": \"vt\",\n",
    "    \"Giải trí\": \"dv\",\n",
    "    \"Quản lý chất lượng (QA/QC)\": \"hc_ql\",\n",
    "    \"Đồ gỗ\": \"cn_sx\",\n",
    "    \"Hóa học\": \"kh_kt\",\n",
    "    \"Nội ngoại thất\": \"tk_kt_nt\",\n",
    "    \"Nhân sự\": \"hc_ql\",\n",
    "    \"Hàng hải\": \"vt\",\n",
    "    \"Dược phẩm\": \"yt_sk\",\n",
    "    \"Vận chuyển / Giao nhận /Kho vận\": \"vt\",\n",
    "    \"Thống kê\": \"pt_tk\",\n",
    "    \"Tiếp thị / Marketing\": \"tt_mkt\",\n",
    "    \"Cơ khí / Ô tô / Tự động hóa\": \"kh_kt\",\n",
    "    \"Xây dựng\": \"xd\",\n",
    "    \"Biên phiên dịch\": \"dv\",\n",
    "    \"Bảo hiểm\": \"law\",\n",
    "    \"Truyền hình / Báo chí / Biên tập\": \"tt_mkt\",\n",
    "    \"Ngành khác\": \"other\",\n",
    "    \"Bưu chính viễn thông\": \"tt_mkt\",\n",
    "    \"Thu mua / Vật tư\": \"kd\",\n",
    "    \"Dệt may / Da giày / Thời trang\": \"cn_sx\",\n",
    "    \"Mới tốt nghiệp / Thực tập\": \"other\",\n",
    "    \"Lao động phổ thông\": \"ldpt\",\n",
    "    \"Thực phẩm & Đồ uống\": \"tp\",\n",
    "    \"Tiếp thị trực tuyến\": \"tt_mkt\",\n",
    "    \"Khác\": \"other\",\n",
    "    \"Thư viện\": \"hc_ql\",\n",
    "    \"Khoáng sản\": \"cn_sx\"\n",
    "}\n",
    "\n",
    "def parse_category(categories):\n",
    "    if categories is None or len(categories) == 0:\n",
    "        return []\n",
    "    else :\n",
    "        category_arr = []\n",
    "        for category in categories:\n",
    "            if category in category_mapping:\n",
    "                category_arr.append(category_mapping[category])\n",
    "            else:\n",
    "                print(\"No result field: \"+ category)\n",
    "                category_arr.append(\"other\")\n",
    "        return category_arr\n",
    "\n",
    "# df.select(\"title\").show(5)\n",
    "rdd = df.rdd.map(lambda row: {\n",
    "    **row.asDict(),\n",
    "    \"salary\": parse_salary(row[\"salary\"]),\n",
    "    \"age\": parse_age(row[\"age\"]),\n",
    "    \"location\": normalize_location(row[\"province\"], row[\"location\"]),\n",
    "    \"experience\": parse_experience(row[\"experience\"]),\n",
    "    \"field\": parse_category(row[\"category\"])\n",
    "})\n",
    "\n",
    "\n",
    "# rdd.count()\n",
    "\n",
    "# df.select([\"role\", \"url\"]).show(30, False)\n",
    "# df.select(\"location\").show(1, False)\n",
    "\n",
    "# rdd.collect()\n",
    "\n",
    "# Insert dữ liệu vào collection \"jobs\"\n",
    "def insert_into_mongodb(partition):\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\", username='admin', password='20194856')\n",
    "    db = client[\"thesis\"]\n",
    "    collection = db[\"jobs\"]\n",
    "\n",
    "    for record in partition:\n",
    "        try:\n",
    "            collection.insert_one(record)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR when insert\"+ str(e))\n",
    "\n",
    "# Insert dữ liệu vào collection \"jobs\"\n",
    "rdd.foreachPartition(insert_into_mongodb)\n",
    "print(\"Insert sucess to mongodb: \"+ str(rdd.count())+ \" jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c7a90-9d72-4f05-8a50-256723c19fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
