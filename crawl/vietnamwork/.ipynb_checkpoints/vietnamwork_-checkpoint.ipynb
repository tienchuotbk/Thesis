{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370fa7cc-6e16-4abe-978f-9472fe05db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/30 11:30:35 WARN Utils: Your hostname, chuot-HP-Pavilion-Laptop-14-ce3xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.14 instead (on interface wlo1)\n",
      "24/06/30 11:30:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/30 11:30:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/30 11:30:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/30 11:30:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')  # Adjust the path if Spark is installed elsewhere\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"vietnamwork\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e6d42a9-d8fc-4066-a8a6-a6f15ab149ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không hiển thị\n",
      "20-40\n",
      "23-30\n",
      "28-33\n",
      "25-40\n",
      "26-35\n",
      "27-45\n",
      "28-40\n",
      "25-45\n",
      "30-50\n",
      "Dưới 30 tuổi\n",
      "25-50\n",
      "22-27\n",
      "20-30\n",
      "24-35\n",
      "23-26\n",
      "26-40\n",
      "15-50\n",
      "20-100\n",
      "22-25\n",
      "24-50\n",
      "25-32\n",
      "24-60\n",
      "25-30\n",
      "24-40\n",
      "20-25\n",
      "30-35\n",
      "35-40\n",
      "35-45\n",
      "27-32\n",
      "23-33\n",
      "30-45\n",
      "15-60\n",
      "23-27\n",
      "24-45\n",
      "22-30\n",
      "21-28\n",
      "27-35\n",
      "21-45\n",
      "Không giới hạn\n",
      "28-55\n",
      "22-28\n",
      "Dưới 35 tuổi\n",
      "24-44\n",
      "20-35\n",
      "24-28\n",
      "27-40\n",
      "22-60\n",
      "22-40\n",
      "22-26\n",
      "22-32\n",
      "20-27\n",
      "28-35\n",
      "25-36\n",
      "26-36\n",
      "30-40\n",
      "25-38\n",
      "22-35\n",
      "25-35\n",
      "20-60\n",
      "18-40\n",
      "25-37\n",
      "25-60\n",
      "24-30\n",
      "23-35\n",
      "None ne\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your JSON file\n",
    "from pyspark.sql.functions import col, expr, date_format, udf, to_date, when, regexp_extract, explode, col, regexp_replace, to_timestamp, concat, lit, array, trim\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, ArrayType\n",
    "import json\n",
    "import re\n",
    "import pymongo\n",
    "json_file_path = 'vietnamwork.json'\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "\n",
    "# df_exploded = df.select(explode(col(\"category\")).alias(\"unique_category\"))\n",
    "\n",
    "# unique_categories = df_exploded.select(\"unique_category\").distinct().collect()\n",
    "# print(len(unique_categories))\n",
    "# for row in unique_categories:\n",
    "    # pass\n",
    "    # print(row.unique_category)\n",
    "# unique_experience_df = df.select(\"location\").distinct()\n",
    "\n",
    "# unique_experience_df.show()\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "unique_values = df.select(col(\"age\")).distinct().collect()\n",
    "\n",
    "# In ra các giá trị duy nhất\n",
    "for row in unique_values:\n",
    "    if row.age is None:\n",
    "        print(\"None ne\")\n",
    "    print(row.age)\n",
    "\n",
    "\n",
    "# print(df.size)\n",
    "\n",
    "\n",
    "#rdd.toDF().select(\"salary\").show(1)\n",
    "\n",
    "# Salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6a971ac-af9d-454f-94ef-472ee6a852b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/30 14:37:14 ERROR Executor: Exception in task 0.0 in stage 129.0 (TID 120)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6262/724100250.py\", line 386, in <lambda>\n",
      "  File \"/tmp/ipykernel_6262/724100250.py\", line 21, in parse_salary\n",
      "ValueError: could not convert string to float: 'Từ 300'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/30 14:37:14 WARN TaskSetManager: Lost task 0.0 in stage 129.0 (TID 120) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6262/724100250.py\", line 386, in <lambda>\n",
      "  File \"/tmp/ipykernel_6262/724100250.py\", line 21, in parse_salary\n",
      "ValueError: could not convert string to float: 'Từ 300'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/06/30 14:37:14 ERROR TaskSetManager: Task 0 in stage 129.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 129.0 failed 1 times, most recent failure: Lost task 0.0 in stage 129.0 (TID 120) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6262/724100250.py\", line 386, in <lambda>\n  File \"/tmp/ipykernel_6262/724100250.py\", line 21, in parse_salary\nValueError: could not convert string to float: 'Từ 300'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6262/724100250.py\", line 386, in <lambda>\n  File \"/tmp/ipykernel_6262/724100250.py\", line 21, in parse_salary\nValueError: could not convert string to float: 'Từ 300'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6262/724100250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparseType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m })\n\u001b[0;32m--> 392\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;31m# def insert_into_mongodb(partition):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 129.0 failed 1 times, most recent failure: Lost task 0.0 in stage 129.0 (TID 120) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6262/724100250.py\", line 386, in <lambda>\n  File \"/tmp/ipykernel_6262/724100250.py\", line 21, in parse_salary\nValueError: could not convert string to float: 'Từ 300'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6262/724100250.py\", line 386, in <lambda>\n  File \"/tmp/ipykernel_6262/724100250.py\", line 21, in parse_salary\nValueError: could not convert string to float: 'Từ 300'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "# Salary\n",
    "def parse_salary(salary):\n",
    "    vnd_per_usd = 25\n",
    "    if salary is None:\n",
    "        return {\"type\": 0 }\n",
    "    elif (salary == \"Thương lượng\"):\n",
    "        return {\"type\": 3 }\n",
    "    elif \"$\" in salary:\n",
    "        # salary = salary.replace(\"$\", \"\").replace(\",\", \".\").strip()\n",
    "        if \"-\" in salary:\n",
    "            min_salary, max_salary = salary.split(\"-\")\n",
    "            min_salary = min_salary.replace(\"$\", \"\").strip()\n",
    "            max_salary = max_salary.replace(\"$\", \"\").strip()\n",
    "            return {\"type\": 1, \"min\": float(min_salary) * vnd_per_usd/1000, \"max\": float(max_salary) * vnd_per_usd/1000}\n",
    "        elif \"Tới\" in salary:\n",
    "            max_salary = salary.replace(\"Tới\", \"\").replace(\"$\", \"\").strip()\n",
    "            return {\"type\": 4, \"max\": float(max_salary) * vnd_per_usd/1000}\n",
    "        elif \"Từ\" in salary:\n",
    "            min_salary = salary.replace(\"Tới\", \"\").replace(\"$\", \"\").strip()\n",
    "            return {\"type\": 3, \"max\": float(min_salary) * vnd_per_usd/1000}\n",
    "        elif \"Từ\" in salary:\n",
    "            fixed_value = salary.replace(\"$\", \"\").replace(\"Từ\", \"\").strip()\n",
    "            return {\"type\": 2, \"fixed\": float(fixed_value) * vnd_per_usd/1000 }\n",
    "\n",
    "# Age\n",
    "def parse_age(age):\n",
    "    if age is None or age == 'Không giới hạn' or age == \"Không hiển thị\":\n",
    "        return { \"type\": 0 }\n",
    "    age = age.strip()\n",
    "    if \"tuổi\" in age:\n",
    "        age = age.replace(\"tuổi\", \"\").replace('Dưới', '').strip()\n",
    "        return { \"type\": 3, \"max\": int(age) }\n",
    "    elif \"-\" in age:\n",
    "        min_age, max_age = age.split(\"-\")\n",
    "        return { \"type\": 1, \"min\": int(min_age), \"max\": int(max_age) }\n",
    "    return { \"type\": 0 }\n",
    "\n",
    "# Experience\n",
    "def parse_experience(experience):\n",
    "    if experience is None or experience == \"Không yêu cầu\" or experience == \"Không yêu cầu\":\n",
    "        return { \"type\": 0 }\n",
    "    else:\n",
    "        fixed = experience.strip()\n",
    "        return { \"type\": 2, \"fixed\": int(fixed)}\n",
    "    # return { \"type\": 0 }\n",
    "    \n",
    "# Type\n",
    "def parseType():\n",
    "    return { \"type\": 0 }\n",
    "\n",
    "# Type\n",
    "df = df.withColumn(\n",
    "    \"certificate\",\n",
    "     when(col(\"certificate\") == None, 0)\n",
    "    .when(col(\"certificate\") == \"Chứng chỉ\", 1)\n",
    "    .when(col(\"certificate\") == \"Trung học\", 2)\n",
    "    .when(col(\"certificate\") == \"Trung cấp\", 3)\n",
    "    .when(col(\"certificate\") == \"Cao đẳng\", 4)\n",
    "    .when(col(\"certificate\") == \"Cử nhân\", 5)\n",
    "    .when(col(\"certificate\") == \"Tiến sĩ\", 6)\n",
    "    .when(col(\"certificate\") == \"Thạc Sĩ\", 6)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Sex\n",
    "df = df.withColumn(\n",
    "    \"sex\",\n",
    "     when(col(\"sex\") == \"Nam\", 1)\n",
    "    .when(col(\"sex\") == \"Nữ\", 2)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "days_pattern = r'Hết hạn trong (\\d+) ngày'\n",
    "df = df.withColumn(\"days_to_add\", regexp_extract(col(\"expiration\"), days_pattern, 1).cast(\"int\"))\n",
    "df = df.withColumn(\"update_time\", to_date(col(\"update_time\"), \"dd/MM/yyyy\"))\n",
    "df = df.withColumn(\"expiration\", expr(\"date_add(update_time, days_to_add)\"))\n",
    "\n",
    "# Expiration\n",
    "df = df.withColumn(\"expiration\", date_format(col(\"expiration\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "# Update at\n",
    "df = df.withColumn(\"update_time\", date_format(col(\"update_time\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "df = df.drop(col(\"days_to_add\"))\n",
    "df = df.drop(col(\"crawl_time\"))\n",
    "df = df.drop(col(\"id\"))\n",
    "\n",
    "\n",
    "# Description\n",
    "def clean_description(description):\n",
    "    return [re.sub(r'^[\\-\\•\\d\\.\\s]+', '', desc).strip() for desc in description]\n",
    "clean_description_udf = udf(clean_description, ArrayType(StringType()))\n",
    "df = df.withColumn(\"description\", clean_description_udf(col(\"description\")))\n",
    "\n",
    "# Role\n",
    "def classify_role(role):\n",
    "    if role == \"Nhân viên\" or role == 'Fresher/Entry level':\n",
    "        return 0\n",
    "    elif role == \"Giám Đốc và Cấp Cao Hơn\":\n",
    "        return 2\n",
    "    elif role == \"Cộng tác viên\":\n",
    "        return 6\n",
    "    elif role == \"Trưởng phòng\":\n",
    "        return 5\n",
    "    elif role == \"Thực tập sinh/Sinh viên\" or role == 'Mới Tốt Nghiệp':\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "classify_role_udf = udf(classify_role, IntegerType())\n",
    "df = df.withColumn(\"role\", classify_role_udf(col(\"role\")))\n",
    "\n",
    "def normalize_benefit(benefits):\n",
    "    return list(map(lambda benefit: re.sub(r'^[*\\-+]+', '', benefit).strip(), benefits))\n",
    "\n",
    "benefit_udf = udf(normalize_benefit, ArrayType(StringType()))\n",
    "df = df.withColumn(\"benefit\", benefit_udf(col(\"benefit\")))\n",
    "df = df.withColumn(\"requirement\", benefit_udf(col(\"requirement\")))\n",
    "\n",
    "def getProvince(province):\n",
    "    if province == \"An Giang\": \n",
    "      location = \"AG\"\n",
    "      \n",
    "    elif province == \"Bà Rịa - Vũng Tàu\": \n",
    "      location = \"BV\"\n",
    "      \n",
    "    elif province == \"Bạc Liêu\": \n",
    "      location = \"BL\"\n",
    "      \n",
    "    elif province == \"Bắc Kạn\" or province == 'Bắc Cạn': \n",
    "      location = \"BK\"\n",
    "      \n",
    "    elif province == \"Bắc Giang\": \n",
    "      location = \"BG\"\n",
    "      \n",
    "    elif province == \"Bắc Ninh\": \n",
    "      location = \"BN\"\n",
    "      \n",
    "    elif province == \"Bến Tre\": \n",
    "      location = \"BT\"\n",
    "      \n",
    "    elif province == \"Bình Dương\": \n",
    "      location = \"BD\"\n",
    "      \n",
    "    elif province == \"Bình Định\": \n",
    "      location = \"BDI\"\n",
    "      \n",
    "    elif province == \"Bình Phước\": \n",
    "      location = \"BP\"\n",
    "      \n",
    "    elif province == \"Bình Thuận\": \n",
    "      location = \"BT\"\n",
    "      \n",
    "    elif province == \"Cà Mau\": \n",
    "      location = \"CM\"\n",
    "      \n",
    "    elif province == \"Cao Bằng\": \n",
    "      location = \"CB\"\n",
    "      \n",
    "    elif province == \"Cần Thơ\": \n",
    "      location = \"CT\"\n",
    "      \n",
    "    elif province == \"Đà Nẵng\": \n",
    "      location = \"DN\"\n",
    "      \n",
    "    elif province == \"Đắk Lắk\" or province == 'Dak Lak': \n",
    "      location = \"DL\"\n",
    "      \n",
    "    elif province == \"Đắk Nông\" or province == 'Dak Nông': \n",
    "      location = \"DNO\"\n",
    "      \n",
    "    elif province == \"Điện Biên\": \n",
    "      location = \"DB\"\n",
    "      \n",
    "    elif province == \"Đồng Nai\": \n",
    "      location = \"DNA\"\n",
    "      \n",
    "    elif province == \"Đồng Tháp\": \n",
    "      location = \"DT\"\n",
    "      \n",
    "    elif province == \"Gia Lai\": \n",
    "      location = \"GL\"\n",
    "      \n",
    "    elif province == \"Hà Giang\": \n",
    "      location = \"HG\"\n",
    "      \n",
    "    elif province == \"Hà Nam\": \n",
    "      location = \"HNA\"\n",
    "      \n",
    "    elif province == \"Hà Nội\": \n",
    "      location = \"HN\"\n",
    "      \n",
    "    elif province == \"Hà Tĩnh\": \n",
    "      location = \"HT\"\n",
    "      \n",
    "    elif province == \"Hải Dương\": \n",
    "      location = \"HD\"\n",
    "      \n",
    "    elif province == \"Hải Phòng\": \n",
    "      location = \"HP\"\n",
    "      \n",
    "    elif province == \"Hậu Giang\": \n",
    "      location = \"HGI\"\n",
    "      \n",
    "    elif province == \"Hòa Bình\": \n",
    "      location = \"HB\"\n",
    "\n",
    "    elif province == 'Hồ Chí Minh' or province == \"TP.HCM\" : \n",
    "      location = \"HCM\"\n",
    "      \n",
    "    elif province == \"Hưng Yên\": \n",
    "      location = \"HY\"\n",
    "      \n",
    "    elif province == \"Khánh Hòa\": \n",
    "      location = \"KH\"\n",
    "      \n",
    "    elif province == \"Kiên Giang\": \n",
    "      location = \"KG\"\n",
    "      \n",
    "    elif province == \"Kon Tum\": \n",
    "      location = \"KT\"\n",
    "      \n",
    "    elif province == \"Lai Châu\": \n",
    "      location = \"LC\"\n",
    "      \n",
    "    elif province == \"Lạng Sơn\": \n",
    "      location = \"LS\"\n",
    "      \n",
    "    elif province == \"Lào Cai\": \n",
    "      location = \"LCA\"\n",
    "      \n",
    "    elif province == \"Lâm Đồng\": \n",
    "      location = \"LD\"\n",
    "      \n",
    "    elif province == \"Long An\": \n",
    "      location = \"LA\"\n",
    "      \n",
    "    elif province == \"Nam Định\": \n",
    "      location = \"ND\"\n",
    "      \n",
    "    elif province == \"Nghệ An\": \n",
    "      location = \"NA\"\n",
    "      \n",
    "    elif province == \"Ninh Bình\": \n",
    "      location = \"NB\"\n",
    "      \n",
    "    elif province == \"Ninh Thuận\": \n",
    "      location = \"NT\"\n",
    "      \n",
    "    elif province == \"Phú Thọ\": \n",
    "      location = \"PT\"\n",
    "      \n",
    "    elif province == \"Phú Yên\": \n",
    "      location = \"PY\"\n",
    "      \n",
    "    elif province == \"Quảng Bình\": \n",
    "      location = \"QB\"\n",
    "      \n",
    "    elif province == \"Quảng Nam\": \n",
    "      location = \"QNA\"\n",
    "      \n",
    "    elif province == \"Quảng Ngãi\": \n",
    "      location = \"QNG\"\n",
    "      \n",
    "    elif province == \"Quảng Ninh\": \n",
    "      location = \"QN\"\n",
    "      \n",
    "    elif province == \"Quảng Trị\": \n",
    "      location = \"QT\"\n",
    "      \n",
    "    elif province == \"Sóc Trăng\": \n",
    "      location = \"ST\"\n",
    "      \n",
    "    elif province == \"Sơn La\": \n",
    "      location = \"SL\"\n",
    "      \n",
    "    elif province == \"Tây Ninh\": \n",
    "      location = \"TN\"\n",
    "      \n",
    "    elif province == \"Thái Bình\": \n",
    "      location = \"TB\"\n",
    "      \n",
    "    elif province == \"Thái Nguyên\": \n",
    "      location = \"TNG\"\n",
    "      \n",
    "    elif province == \"Thanh Hóa\": \n",
    "      location = \"TH\"\n",
    "      \n",
    "    elif province == \"Thừa Thiên- Huế\" or province == \"Thừa Thiên Huế\": \n",
    "      location = \"TTH\"\n",
    "      \n",
    "    elif province == \"Tiền Giang\": \n",
    "      location = \"TG\"\n",
    "      \n",
    "    elif province == \"Trà Vinh\": \n",
    "      location = \"TV\"\n",
    "      \n",
    "    elif province == \"Tuyên Quang\": \n",
    "      location = \"TQ\"\n",
    "      \n",
    "    elif province == \"Vĩnh Long\": \n",
    "      location = \"VL\"\n",
    "      \n",
    "    elif province == \"Vĩnh Phúc\": \n",
    "      location = \"VP\"\n",
    "      \n",
    "    elif province == \"Yên Bái\": \n",
    "      location = \"YB\"\n",
    "    else:\n",
    "        print(\"No match:\\\"\"+ province+ \"\\\"\")\n",
    "        location = 'other'\n",
    "    return location\n",
    "\n",
    "# location\n",
    "def normalize_location(provinces, locations):\n",
    "    arr = []\n",
    "    if provinces == None or locations == None:\n",
    "        return []\n",
    "    provinces = provinces.split(',')\n",
    "    for province, location in zip(provinces, locations):\n",
    "        parts = [part.strip() for part in location.split(',') if part.strip()]\n",
    "        if len(parts) >= 2:\n",
    "            job_province = parts[-1]\n",
    "            district = None\n",
    "            if job_province == province:\n",
    "                district = parts[-2]\n",
    "            else:\n",
    "                district = [address for address in parts if 'huyện' in address.lower() or 'Q.' in address or 'Quận' in address.lower()]\n",
    "            arr.append({\"province\": getProvince(province), \"district\": district, \"address\": location})\n",
    "        else: \n",
    "            arr.append({\"address\": location, \"district\": None, \"province\": getProvince(province)})\n",
    "    return arr\n",
    "\n",
    "df = df.withColumn(\"category\", array(col(\"category\"), col(\"field\")))\n",
    "\n",
    "category_mapping = {\n",
    "    \"Kế Toán/Kiểm Toán\": \"tc_kt\",\n",
    "    \"Bảo Hiểm\": \"dv\",\n",
    "    \"Khác\": \"other\",\n",
    "    \"Công Nghệ Thông Tin/Viễn Thông\": \"it\",\n",
    "    \"Dệt May/Da Giày\": \"cn_sx\",\n",
    "    \"Banking & Financial Services\": \"tc_kt\",\n",
    "    \"Thiết Kế\": \"tk_kt_nt\",\n",
    "    \"Kỹ Thuật\": \"kh_kt\",\n",
    "    \"Nhân Sự/Tuyển Dụng\": \"hc_ql\",\n",
    "    \"Pháp Lý\": \"law\",\n",
    "    \"Dịch Vụ Ăn Uống\": \"tp\",\n",
    "    \"Nghệ thuật, Truyền thông/In ấn/Xuất bản\": \"xb\",\n",
    "    \"Chính Phủ/Phi Lợi Nhuận\": \"other\",\n",
    "    \"Hậu Cần/Xuất Nhập Khẩu/Kho Bãi\": \"vt\",\n",
    "    \"Kinh Doanh\": \"kd\",\n",
    "    \"Y Tế/Chăm Sóc Sức Khoẻ\": \"yt_sk\",\n",
    "    \"Vận Tải\": \"vt\",\n",
    "    \"Bán Lẻ/Tiêu Dùng\": \"kd\",\n",
    "    \"Hành Chính Văn Phòng\": \"hc_ql\",\n",
    "    \"Ngân Hàng & Dịch Vụ Tài Chính\": \"tc_kt\",\n",
    "    \"Kiến Trúc/Xây Dựng\": \"xd\",\n",
    "    \"Tiếp Thị, Quảng Cáo/Truyền Thông\": \"tt_mkt\",\n",
    "    \"Dịch Vụ Khách Hàng\": \"dv\",\n",
    "    \"CEO & General Management\": \"hc_ql\",\n",
    "    \"Nông/Lâm/Ngư Nghiệp\": \"nn_ln_ts\",\n",
    "    \"Sản Xuất\": \"cn_sx\",\n",
    "    \"Nhà Hàng - Khách Sạn/Du Lịch\": \"dv\",\n",
    "    \"Khoa Học & Kỹ Thuật\": \"kh_kt\",\n",
    "    \"Giáo Dục\": \"gd_dt\",\n",
    "    \"Dược\": \"yt_sk\",\n",
    "    \"Cung cấp nhân lực\": \"hc_ql\",\n",
    "    \"Bất Động Sản\": \"bds\"\n",
    "}\n",
    "\n",
    "def parse_category(categories):\n",
    "    if categories is None:\n",
    "        return []\n",
    "    else :\n",
    "        field = category_mapping[category]\n",
    "        if field is not None:\n",
    "            return [ field ]\n",
    "        else:\n",
    "            return [\"other\"]\n",
    "\n",
    "df = df.drop(col(\"requirement\"))\n",
    "df = df.drop(col(\"benefit\"))\n",
    "df = df.drop(col(\"description\"))\n",
    "# df.show(1, truncate=False)\n",
    "# df.printSchema()\n",
    "rdd = df.rdd.map(lambda row: {\n",
    "    **row.asDict(),\n",
    "    \"salary\": parse_salary(row[\"salary\"]),\n",
    "    \"age\": parse_age(row[\"age\"]),\n",
    "    # \"location\": normalize_location(row[\"location\"]),\n",
    "    # \"experience\": parse_experience(row[\"experience\"]),\n",
    "    \"type\": parseType()\n",
    "})\n",
    "print(rdd.collect())\n",
    "\n",
    "# def insert_into_mongodb(partition):\n",
    "#     client = pymongo.MongoClient(\"mongodb://localhost:27017/\", username='admin', password='20194856')\n",
    "#     db = client[\"thesis\"]\n",
    "#     collection = db[\"jobs\"]\n",
    "\n",
    "#     for record in partition:\n",
    "#         try:\n",
    "#             collection.insert_one(record)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "\n",
    "# # # Insert dữ liệu vào collection \"jobs\"\n",
    "# rdd.foreachPartition(insert_into_mongodb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c7a90-9d72-4f05-8a50-256723c19fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1563b9-76cf-4527-91ef-7b70cb27a1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
