{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370fa7cc-6e16-4abe-978f-9472fe05db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/30 15:55:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')  # Adjust the path if Spark is installed elsewhere\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"topcv\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e6d42a9-d8fc-4066-a8a6-a6f15ab149ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the path to your JSON file\n",
    "from pyspark.sql.functions import col, struct, regexp_extract, udf, date_sub, to_date, when, regexp_extract, expr, explode, col, regexp_replace, to_timestamp, concat, lit, trim, date_format\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, ArrayType\n",
    "import json\n",
    "import re\n",
    "import pymongo\n",
    "json_file_path = 'topcvnew.json'\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "\n",
    "# df_exploded = df.select(explode(col(\"category\")).alias(\"unique_category\"))\n",
    "# unique_categories = df_exploded.select(\"unique_category\").distinct().collect()\n",
    "# print(len(unique_categories))\n",
    "# for row in unique_categories:\n",
    "#     # pass\n",
    "#     print(row.unique_category)\n",
    "    # parts = [part.strip() for part in row.unique_category.split(',')]\n",
    "    # print(parts)\n",
    "\n",
    "# df_exploded = df.select(explode(col(\"category\")).alias(\"unique_category\"))\n",
    "\n",
    "# unique_categories = df_exploded.select(\"unique_category\").distinct().collect()\n",
    "# print(len(unique_categories))\n",
    "# for row in unique_categories:\n",
    "#     pass\n",
    "#     print(row.unique_category)\n",
    "# unique_experience_df = df.select(\"location\").distinct()\n",
    "\n",
    "# unique_experience_df.show()\n",
    "# from pyspark.sql.functions import col\n",
    "# unique_values = df.select(col(\"category\")).distinct().collect()\n",
    "# In ra các giá trị duy nhất\n",
    "# for row in unique_values:\n",
    "#     if row.category is None:\n",
    "#         print(\"None ne\")\n",
    "#     print(row.category)\n",
    "\n",
    "\n",
    "# Salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6a971ac-af9d-454f-94ef-472ee6a852b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/30 15:55:59 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/tmp/ipykernel_5334/2027065263.py\", line 485, in insert_into_mongodb\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5334/2027065263.py\", line 465, in <lambda>\n",
      "  File \"/tmp/ipykernel_5334/2027065263.py\", line 362, in normalize_location\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9563, in regexp_extract\n",
      "    return _invoke_function(\"regexp_extract\", _to_java_column(str), pattern, idx)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n",
      "    jcol = _create_column_from_name(col)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/06/30 15:55:59 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/tmp/ipykernel_5334/2027065263.py\", line 485, in insert_into_mongodb\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5334/2027065263.py\", line 465, in <lambda>\n",
      "  File \"/tmp/ipykernel_5334/2027065263.py\", line 362, in normalize_location\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9563, in regexp_extract\n",
      "    return _invoke_function(\"regexp_extract\", _to_java_column(str), pattern, idx)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n",
      "    jcol = _create_column_from_name(col)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n",
      "    sc = get_active_spark_context()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n",
      "    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\n",
      "RuntimeError: SparkContext or SparkSession should be created first.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/06/30 15:55:59 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 485, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 465, in <lambda>\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 362, in normalize_location\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9563, in regexp_extract\n    return _invoke_function(\"regexp_extract\", _to_java_column(str), pattern, idx)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n    jcol = _create_column_from_name(col)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n    sc = get_active_spark_context()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 485, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 465, in <lambda>\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 362, in normalize_location\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9563, in regexp_extract\n    return _invoke_function(\"regexp_extract\", _to_java_column(str), pattern, idx)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n    jcol = _create_column_from_name(col)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n    sc = get_active_spark_context()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5334/2027065263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;31m# # Insert dữ liệu vào collection \"jobs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minsert_into_mongodb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;31m# spark.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mforeachPartition\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1801\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[T]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2314\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \"\"\"\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \"\"\"\n\u001b[0;32m-> 2291\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   2292\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m         )\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.14 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 485, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 465, in <lambda>\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 362, in normalize_location\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9563, in regexp_extract\n    return _invoke_function(\"regexp_extract\", _to_java_column(str), pattern, idx)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n    jcol = _create_column_from_name(col)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n    sc = get_active_spark_context()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/opt/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/opt/spark/python/pyspark/rdd.py\", line 1795, in func\n    r = f(it)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 485, in insert_into_mongodb\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 465, in <lambda>\n  File \"/tmp/ipykernel_5334/2027065263.py\", line 362, in normalize_location\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 9563, in regexp_extract\n    return _invoke_function(\"regexp_extract\", _to_java_column(str), pattern, idx)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n    jcol = _create_column_from_name(col)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n    sc = get_active_spark_context()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(json_file_path)\n",
    "df.dropDuplicates()\n",
    "df = df.filter(col(\"company\").isNotNull())\n",
    "date_pattern = r'(\\d{2}/\\d{2}/\\d{4})'\n",
    "\n",
    "df = df.withColumn(\"certificate\", lit(0))\n",
    "# df = df.withColumn(\"age\", struct(lit(0).alias(\"type\")))\n",
    "df = df.withColumn(\n",
    "    \"sex\",\n",
    "     when(col(\"sex\") == \"Nam\", 1)\n",
    "    .when(col(\"sex\") == \"Nữ\", 2)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Experience\n",
    "def parse_experience(experience):\n",
    "    if experience is None or experience == \"Không yêu cầu kinh nghiệm\":\n",
    "        return { \"type\": 0 }\n",
    "    if \"-\" in experience:\n",
    "        strip_experience = experience.replace(\"Năm\", \"\").strip()\n",
    "        min_exp, max_exp = strip_experience.split(\"-\")\n",
    "        return {\"type\": 1, \"min\": int(min_exp), \"max\": int(max_exp) }\n",
    "    elif \"Dưới\" in experience:\n",
    "        strip_experience = experience.replace(\"Dưới\", \"\").replace(\"năm\", \"\").strip()\n",
    "        max = strip_experience\n",
    "        return { \"type\": 3, \"max\": int(max) }\n",
    "    elif \"Trên\" in experience:\n",
    "        strip_experience = experience.replace(\"Trên\", \"\").replace(\"năm\", \"\").strip()\n",
    "        min = strip_experience\n",
    "        return { \"type\": 4, \"min\": int(min) }\n",
    "    else:\n",
    "        fixed = experience.replace(\"năm\", \"\").strip()\n",
    "        return { \"type\": 2, \"fixed\": int(fixed)}\n",
    "    return { \"type\": 0 }\n",
    "# Type\n",
    "def get_type_job(type):\n",
    "    if type == \"Toàn thời gian\":\n",
    "        return 0\n",
    "    elif type == \"Bán thời gian\":\n",
    "        return 2\n",
    "    elif type == \"Thực tập\":\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def classify_role(role):\n",
    "    if role is None:\n",
    "        return 0\n",
    "    role = role.strip()\n",
    "    if role == \"Nhân viên\":\n",
    "        return 0\n",
    "    elif role == \"Quản lý / Giám sát\" or role == \"Trưởng chi nhánh\":\n",
    "        return 1\n",
    "    elif role == \"Cộng tác viên\":\n",
    "        return 6\n",
    "    elif role == \"Trưởng nhóm\" or role == \"Trưởng/Phó phòng\" :\n",
    "        return 5\n",
    "    elif role == \"Thực tập sinh\":\n",
    "        return 4\n",
    "    elif role == \"Giám đốc\" or role == \"Tổng giám đốc\":\n",
    "        return 2\n",
    "    elif role == \"Phó giám đốc\":\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "classify_role_udf = udf(classify_role, IntegerType())\n",
    "df = df.withColumn(\"role\", classify_role_udf(col(\"role\")))\n",
    "\n",
    "# Salary\n",
    "def parse_salary(salary):\n",
    "    vnd_per_usd = 25\n",
    "    if salary is None:\n",
    "        return {\"type\": 0 }\n",
    "    elif (salary.strip() == \"Thoả thuận\"):\n",
    "        return {\"type\": 3 }\n",
    "    elif \"triệu\" in salary:\n",
    "            salary = salary.replace(\" triệu\", \"\").replace(\",\", \".\").strip()\n",
    "            if \"-\" in salary:\n",
    "                min_salary, max_salary = salary.split(\"-\")\n",
    "                return {\"type\": 1, \"min\": float(min_salary.strip()), \"max\": float(max_salary.strip()) }\n",
    "            elif \"Tới\" in salary:\n",
    "                min_salary = salary.replace(\"Tới\", \"\").strip()\n",
    "                return {\"type\": 5, \"min\": float(min_salary)}\n",
    "            elif \"Trên\" in salary:\n",
    "                max_salary = salary.replace(\"Trên\", \"\").strip()\n",
    "                return {\"type\": 4, \"max\": float(max_salary)}\n",
    "    elif \"USD\" in salary:\n",
    "        salary = salary.replace(\" USD\", \"\").replace(\",\", \"\").strip()\n",
    "        if \"-\" in salary:\n",
    "            min_salary, max_salary = salary.split(\"-\")\n",
    "            return {\"type\": 1, \"min\": float(min_salary)* vnd_per_usd / 1000, \"max\": float(max_salary)* vnd_per_usd /1000}\n",
    "        elif \"Tới\" in salary:\n",
    "            max_salary = salary.replace(\"Tới\", \"\").strip()\n",
    "            return {\"type\": 4, \"max\": float(max_salary)* vnd_per_usd / 1000}\n",
    "        elif \"Trên\" in salary:\n",
    "            min_salary = salary.replace(\"Trên\", \"\").strip()\n",
    "            return {\"type\": 5, \"min\": float(min_salary)* vnd_per_usd / 1000}\n",
    "    else:\n",
    "        return {\"type\": 0 }\n",
    "                \n",
    "# Age\n",
    "def parse_age(): \n",
    "    return { \"type\": 0 }\n",
    "\n",
    "# Extract the date string\n",
    "df = df.withColumn(\"date_str\", regexp_extract(col(\"expiration\"), date_pattern, 1))\n",
    "# Expiration\n",
    "df = df.withColumn(\"expiration\", date_format(to_timestamp(concat(col(\"date_str\"), lit(\" 00:00:00\")), \"dd/MM/yyyy HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Update at\n",
    "hours_pattern = r'(\\d+)\\s+giờ'\n",
    "days_pattern = r'(\\d+)\\s+ngày'\n",
    "weeks_pattern = r'(\\d+)\\s+tuần'\n",
    "# df = df.withColumn(\"update_time\", date_format(to_timestamp(concat(getCrawlTime(col(\"crawl_time\"), col(\"create_time\")), lit(\" 00:00:00\")), \"dd/MM/yyyy HH:mm:ss\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "# Extract the time component and calculate days to subtract\n",
    "df = df.withColumn(\"hours\", regexp_extract(col(\"create_time\"), hours_pattern, 1).cast(\"int\"))\n",
    "df = df.withColumn(\"days\", regexp_extract(col(\"create_time\"), days_pattern, 1).cast(\"int\"))\n",
    "df = df.withColumn(\"weeks\", regexp_extract(col(\"create_time\"), weeks_pattern, 1).cast(\"int\"))\n",
    "\n",
    "# Calculate the total days to subtract\n",
    "df = df.withColumn(\"days_to_subtract\", \n",
    "                   when(col(\"hours\").isNotNull(), 1)\n",
    "                   .when(col(\"days\").isNotNull(), col(\"days\"))\n",
    "                   .when(col(\"weeks\").isNotNull(), col(\"weeks\") * 7)\n",
    "                   .otherwise(0))\n",
    "\n",
    "# Convert crawl_time to date\n",
    "df = df.withColumn(\"crawl_time\", to_date(col(\"crawl_time\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Calculate update_time by subtracting days_to_subtract from crawl_time\n",
    "df = df.withColumn(\"update_time\", expr(\"date_sub(crawl_time, days_to_subtract)\"))\n",
    "df = df.drop(col(\"create_time\"))\n",
    "df = df.drop(col(\"crawl_time\"))\n",
    "df = df.drop(col(\"days_to_subtract\"))\n",
    "\n",
    "# df.select(\"crawl_time\", \"create_time\", \"update_time\").show(truncate=False)\n",
    "def get_list_type_job(types):\n",
    "    # Thời vụ/ Nghề tự do, Thực tập\n",
    "    # type = types.split(\",\")\n",
    "    if types is None:\n",
    "        return []\n",
    "    return [get_type_job(word.strip()) for word in types.split(\",\")]\n",
    "\n",
    "\n",
    "classify_type_udf = udf(get_list_type_job, ArrayType(IntegerType()))\n",
    "df = df.withColumn(\"type\", classify_type_udf(col(\"type\")))\n",
    "\n",
    "def clean_description(description):\n",
    "    if description is None:\n",
    "        return []\n",
    "    # Filter out empty strings and None values\n",
    "    cleaned_descriptions = [desc for desc in description if desc is not None and len(desc.strip())> 0]\n",
    "    # Apply cleaning to filtered descriptions\n",
    "    cleaned_descriptions = [re.sub(r'^[\\-\\•\\d\\.\\s]+', '', desc).strip() for desc in cleaned_descriptions]\n",
    "    return cleaned_descriptions\n",
    "clean_description_udf = udf(clean_description, ArrayType(StringType()))\n",
    "df = df.withColumn(\"description\", clean_description_udf(col(\"description\")))\n",
    "df = df.withColumn(\"benefit\", clean_description_udf(col(\"benefit\")))\n",
    "df = df.withColumn(\"requirement\", clean_description_udf(col(\"requirement\")))\n",
    "\n",
    "def getProvince(province):\n",
    "    if province == \"An Giang\": \n",
    "      location = \"AG\"\n",
    "      \n",
    "    elif province == \"Bà Rịa - Vũng Tàu\": \n",
    "      location = \"BV\"\n",
    "      \n",
    "    elif province == \"Bạc Liêu\": \n",
    "      location = \"BL\"\n",
    "      \n",
    "    elif province == \"Bắc Kạn\" or province == 'Bắc Cạn': \n",
    "      location = \"BK\"\n",
    "      \n",
    "    elif province == \"Bắc Giang\": \n",
    "      location = \"BG\"\n",
    "      \n",
    "    elif province == \"Bắc Ninh\": \n",
    "      location = \"BN\"\n",
    "      \n",
    "    elif province == \"Bến Tre\": \n",
    "      location = \"BT\"\n",
    "      \n",
    "    elif province == \"Bình Dương\": \n",
    "      location = \"BD\"\n",
    "      \n",
    "    elif province == \"Bình Định\": \n",
    "      location = \"BDI\"\n",
    "      \n",
    "    elif province == \"Bình Phước\": \n",
    "      location = \"BP\"\n",
    "      \n",
    "    elif province == \"Bình Thuận\": \n",
    "      location = \"BT\"\n",
    "      \n",
    "    elif province == \"Cà Mau\": \n",
    "      location = \"CM\"\n",
    "      \n",
    "    elif province == \"Cao Bằng\": \n",
    "      location = \"CB\"\n",
    "      \n",
    "    elif province == \"Cần Thơ\": \n",
    "      location = \"CT\"\n",
    "      \n",
    "    elif province == \"Đà Nẵng\": \n",
    "      location = \"DN\"\n",
    "      \n",
    "    elif province == \"Đắk Lắk\" or province == 'Dak Lak': \n",
    "      location = \"DL\"\n",
    "      \n",
    "    elif province == \"Đắk Nông\" or province == 'Dak Nông': \n",
    "      location = \"DNO\"\n",
    "      \n",
    "    elif province == \"Điện Biên\": \n",
    "      location = \"DB\"\n",
    "      \n",
    "    elif province == \"Đồng Nai\": \n",
    "      location = \"DNA\"\n",
    "      \n",
    "    elif province == \"Đồng Tháp\": \n",
    "      location = \"DT\"\n",
    "      \n",
    "    elif province == \"Gia Lai\": \n",
    "      location = \"GL\"\n",
    "      \n",
    "    elif province == \"Hà Giang\": \n",
    "      location = \"HG\"\n",
    "      \n",
    "    elif province == \"Hà Nam\": \n",
    "      location = \"HNA\"\n",
    "      \n",
    "    elif province == \"Hà Nội\": \n",
    "      location = \"HN\"\n",
    "      \n",
    "    elif province == \"Hà Tĩnh\": \n",
    "      location = \"HT\"\n",
    "      \n",
    "    elif province == \"Hải Dương\": \n",
    "      location = \"HD\"\n",
    "      \n",
    "    elif province == \"Hải Phòng\": \n",
    "      location = \"HP\"\n",
    "      \n",
    "    elif province == \"Hậu Giang\": \n",
    "      location = \"HGI\"\n",
    "      \n",
    "    elif province == \"Hòa Bình\": \n",
    "      location = \"HB\"\n",
    "\n",
    "    elif province == 'Hồ Chí Minh' or province == \"TP.HCM\" : \n",
    "      location = \"HCM\"\n",
    "      \n",
    "    elif province == \"Hưng Yên\": \n",
    "      location = \"HY\"\n",
    "      \n",
    "    elif province == \"Khánh Hòa\": \n",
    "      location = \"KH\"\n",
    "      \n",
    "    elif province == \"Kiên Giang\": \n",
    "      location = \"KG\"\n",
    "      \n",
    "    elif province == \"Kon Tum\": \n",
    "      location = \"KT\"\n",
    "      \n",
    "    elif province == \"Lai Châu\": \n",
    "      location = \"LC\"\n",
    "      \n",
    "    elif province == \"Lạng Sơn\": \n",
    "      location = \"LS\"\n",
    "      \n",
    "    elif province == \"Lào Cai\": \n",
    "      location = \"LCA\"\n",
    "      \n",
    "    elif province == \"Lâm Đồng\": \n",
    "      location = \"LD\"\n",
    "      \n",
    "    elif province == \"Long An\": \n",
    "      location = \"LA\"\n",
    "      \n",
    "    elif province == \"Nam Định\": \n",
    "      location = \"ND\"\n",
    "      \n",
    "    elif province == \"Nghệ An\": \n",
    "      location = \"NA\"\n",
    "      \n",
    "    elif province == \"Ninh Bình\": \n",
    "      location = \"NB\"\n",
    "      \n",
    "    elif province == \"Ninh Thuận\": \n",
    "      location = \"NT\"\n",
    "      \n",
    "    elif province == \"Phú Thọ\": \n",
    "      location = \"PT\"\n",
    "      \n",
    "    elif province == \"Phú Yên\": \n",
    "      location = \"PY\"\n",
    "      \n",
    "    elif province == \"Quảng Bình\": \n",
    "      location = \"QB\"\n",
    "      \n",
    "    elif province == \"Quảng Nam\": \n",
    "      location = \"QNA\"\n",
    "      \n",
    "    elif province == \"Quảng Ngãi\": \n",
    "      location = \"QNG\"\n",
    "      \n",
    "    elif province == \"Quảng Ninh\": \n",
    "      location = \"QN\"\n",
    "      \n",
    "    elif province == \"Quảng Trị\": \n",
    "      location = \"QT\"\n",
    "      \n",
    "    elif province == \"Sóc Trăng\": \n",
    "      location = \"ST\"\n",
    "      \n",
    "    elif province == \"Sơn La\": \n",
    "      location = \"SL\"\n",
    "      \n",
    "    elif province == \"Tây Ninh\": \n",
    "      location = \"TN\"\n",
    "      \n",
    "    elif province == \"Thái Bình\": \n",
    "      location = \"TB\"\n",
    "      \n",
    "    elif province == \"Thái Nguyên\": \n",
    "      location = \"TNG\"\n",
    "      \n",
    "    elif province == \"Thanh Hóa\": \n",
    "      location = \"TH\"\n",
    "      \n",
    "    elif province == \"Thừa Thiên- Huế\" or province == \"Thừa Thiên Huế\": \n",
    "      location = \"TTH\"\n",
    "      \n",
    "    elif province == \"Tiền Giang\": \n",
    "      location = \"TG\"\n",
    "      \n",
    "    elif province == \"Trà Vinh\": \n",
    "      location = \"TV\"\n",
    "      \n",
    "    elif province == \"Tuyên Quang\": \n",
    "      location = \"TQ\"\n",
    "      \n",
    "    elif province == \"Vĩnh Long\": \n",
    "      location = \"VL\"\n",
    "      \n",
    "    elif province == \"Vĩnh Phúc\": \n",
    "      location = \"VP\"\n",
    "      \n",
    "    elif province == \"Yên Bái\": \n",
    "      location = \"YB\"\n",
    "    else:\n",
    "        print(\"No match:\\\"\"+ province+ \"\\\"\")\n",
    "        location = 'other'\n",
    "    return location\n",
    "\n",
    "# location\n",
    "def normalize_location(locations):\n",
    "    arr = []\n",
    "    province_pattern = r'- ([^:]+):'\n",
    "    if locations == None:\n",
    "        return []\n",
    "    for location in locations:\n",
    "        province = regexp_extract(location, province_pattern, 1)\n",
    "        list_add = split(location, \":\")\n",
    "        if len(list_add) > 2:\n",
    "            address = split(location, \":\")[1].trim()\n",
    "            arr.append({\"province\": getProvince(province), \"district\": None, \"address\": address})\n",
    "        else: \n",
    "            arr.append({\"address\": None, \"district\": None, \"province\": getProvince(province)})\n",
    "    return arr\n",
    "\n",
    "category_mapping = {\n",
    "    \"Du lịch\": \"dv\",\n",
    "    \"Hàng gia dụng\": \"kd\",\n",
    "    \"Địa chất / Khoáng sản\": \"cn_sx\",\n",
    "    \"Sản phẩm công nghiệp\": \"cn_sx\",\n",
    "    \"Hành chính / Văn phòng\": \"hc_ql\",\n",
    "    \"In ấn / Xuất bản\": \"xb\",\n",
    "    \"An toàn lao động\": \"at_an\",\n",
    "    \"Spa / Làm đẹp\": \"dv\",\n",
    "    \"Công nghệ cao\": \"kh_kt\",\n",
    "    \"IT Phần cứng / Mạng\": \"it\",\n",
    "    \"Luật/Pháp lý\": \"law\",\n",
    "    \"Marketing / Truyền thông / Quảng cáo\": \"tt_mkt\",\n",
    "    \"Dầu khí/Hóa chất\": \"cn_sx\",\n",
    "    \"Quản lý điều hành\": \"hc_ql\",\n",
    "    \"Thực phẩm / Đồ uống\": \"tp\",\n",
    "    \"Kiến trúc\": \"tk_kt_nt\",\n",
    "    \"Ngành nghề khác\": \"other\",\n",
    "    \"Thư ký / Trợ lý\": \"hc_ql\",\n",
    "    \"Điện / Điện tử / Điện lạnh\": \"kh_kt\",\n",
    "    \"Kinh doanh / Bán hàng\": \"kd\",\n",
    "    \"Bảo trì / Sửa chữa\": \"kh_kt\",\n",
    "    \"Chứng khoán / Vàng / Ngoại tệ\": \"tc_kt\",\n",
    "    \"Hàng không\": \"vt\",\n",
    "    \"Môi trường / Xử lý chất thải\": \"cn_sx\",\n",
    "    \"Bất động sản\": \"bds\",\n",
    "    \"Tư vấn\": \"dv\",\n",
    "    \"Dịch vụ khách hàng\": \"dv\",\n",
    "    \"Y tế / Chăm sóc sức khỏe\": \"yt_sk\",\n",
    "    \"Mỹ phẩm / Trang sức\": \"kd\",\n",
    "    \"Cơ khí / Chế tạo / Tự động hóa\": \"kh_kt\",\n",
    "    \"Giáo dục / Đào tạo\": \"gd_dt\",\n",
    "    \"Hàng tiêu dùng\": \"kd\",\n",
    "    \"Thiết kế đồ họa\": \"tk_kt_nt\",\n",
    "    \"Bán lẻ / Bán sỉ\": \"kd\",\n",
    "    \"Tài chính / Đầu tư\": \"tc_kt\",\n",
    "    \"IT phần mềm\": \"it\",\n",
    "    \"Sản xuất\": \"cn_sx\",\n",
    "    \"Mỹ thuật / Nghệ thuật / Điện ảnh\": \"tk_kt_nt\",\n",
    "    \"Tổ chức sự kiện / Quà tặng\": \"dv\",\n",
    "    \"Hoạch định/Dự án\": \"xd\",\n",
    "    \"Bán lẻ / bán sỉ\": \"kd\",\n",
    "    \"Kế toán / Kiểm toán\": \"tc_kt\",\n",
    "    \"Xuất nhập khẩu\": \"vt\",\n",
    "    \"Công nghệ Ô tô\": \"cn_sx\",\n",
    "    \"Quản lý chất lượng (QA/QC)\": \"hc_ql\",\n",
    "    \"Biên / Phiên dịch\": \"dv\",\n",
    "    \"Thời trang\": \"tk_kt_nt\",\n",
    "    \"Nông / Lâm / Ngư nghiệp\": \"nn_ln_ts\",\n",
    "    \"Điện tử viễn thông\": \"dv\",\n",
    "    \"Ngân hàng / Tài chính\": \"tc_kt\",\n",
    "    \"Công nghệ thông tin\": \"it\",\n",
    "    \"Nhân sự\": \"hc_ql\",\n",
    "    \"Hàng hải\": \"vt\",\n",
    "    \"Y tế / Dược\": \"yt_sk\",\n",
    "    \"Xây dựng\": \"xd\",\n",
    "    \"Dược phẩm / Công nghệ sinh học\": \"kh_kt\",\n",
    "    \"Bảo hiểm\": \"dv\",\n",
    "    \"Việc làm IT\": \"it\",\n",
    "    \"Khách sạn / Nhà hàng\": \"dv\",\n",
    "    \"Thiết kế nội thất\": \"tk_kt_nt\",\n",
    "    \"Logistics\": \"vt\",\n",
    "    \"Bưu chính - Viễn thông\": \"dv\",\n",
    "    \"Hàng cao cấp\": \"kd\",\n",
    "    \"Vận tải / Kho vận\": \"vt\",\n",
    "    \"Dệt may / Da giày\": \"cn_sx\",\n",
    "    \"Hoá học / Sinh học\": \"kh_kt\",\n",
    "    \"Bán hàng kỹ thuật\": \"kd\",\n",
    "    \"Báo chí / Truyền hình\": \"tt_mkt\"\n",
    "}\n",
    "\n",
    "def parse_category(categories):\n",
    "    if categories is None or len(categories) == 0:\n",
    "        return []\n",
    "    else :\n",
    "        category_arr = []\n",
    "        for category in categories:\n",
    "            if category in category_mapping:\n",
    "                category_arr.append(category_mapping[category])\n",
    "            else:\n",
    "                print(\"No result field: \"+ category)\n",
    "                category_arr.append(\"other\")\n",
    "        return category_arr\n",
    "\n",
    "df = df.drop(col(\"province\"))\n",
    "df = df.drop(col(\"hours\"))\n",
    "df = df.drop(col(\"days\"))\n",
    "df = df.drop(col(\"weeks\"))\n",
    "df = df.drop(col(\"date_str\"))\n",
    "\n",
    "rdd = df.rdd.map(lambda row: {\n",
    "    **row.asDict(),\n",
    "    \"salary\": parse_salary(row[\"salary\"]),\n",
    "    \"age\": parse_age(),\n",
    "    \"location\": normalize_location(row[\"location\"]),\n",
    "    \"experience\": parse_experience(row[\"experience\"]),\n",
    "    \"field\": parse_category(row[\"category\"])\n",
    "})\n",
    "# df.select('expiration').show(5)\n",
    "# print(rdd)\n",
    "# df.printSchema()\n",
    "\n",
    "# df.select([\"role\", \"url\"]).show(30, False)\n",
    "# df.select(\"location\").show(1, False)\n",
    "\n",
    "# rdd.top(5)\n",
    "# rdd.count()\n",
    "\n",
    "# Insert dữ liệu vào collection \"jobs\"\n",
    "def insert_into_mongodb(partition):\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\", username='admin', password='20194856')\n",
    "    db = client[\"thesis\"]\n",
    "    collection = db[\"jobs\"]\n",
    "\n",
    "    for record in partition:\n",
    "        # print(record)\n",
    "        collection.insert_one(record)\n",
    "\n",
    "# # Insert dữ liệu vào collection \"jobs\"\n",
    "rdd.foreachPartition(insert_into_mongodb)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c7a90-9d72-4f05-8a50-256723c19fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
